{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework2_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S2IaITBP0f5R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 2\n",
        "In this homework, we will explore language generation using character-level RNNs. Sounds awesome, right?\n",
        "\n",
        "A few notes at the beginning:\n",
        "- It might be useful for you to read the whole assignment before beginning. Especially the last two sections so you know what to record for turning in.\n",
        "- Much of the required knowledge in this (and past) homeworks about Python, PyTorch, etc. are not explained fully here. Instead, we expect you to use the existing documentation, search engines, Stack Overflow, etc. for implementation details.\n",
        "- That being said, we have listed several functions in parts of the homework where knowing those functions exist would be especially useful. However you will still need to read the docs on how to specifically use the functions.\n",
        "\n",
        "# Part 0: Initial setup\n",
        "You should recognize this code from last time.\n"
      ]
    },
    {
      "metadata": {
        "id": "4GS0yuGl0mHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "5e72179f-cc27-4b58-a0c1-16138ea217e2"
      },
      "cell_type": "code",
      "source": [
        "# This is code to download and install pytorch\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if os.path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (483.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 483.0MB 52.2MB/s \n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n",
            "Version 0.4.1\n",
            "CUDA enabled: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6t3ZIEll0pr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "95aa976b-ae40-449b-efbb-069ea4b33e0d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PEzPNAIY0vkm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Upload the dataset\n",
        "We will be using the complete text of Harry Potter as our corpus. We will provide it for you in a not-very-well-formatted way.\n",
        "Run this code to navigate to the BASE_PATH directory and upload the homework2.tar file inside the BASE_PATH, then extract it.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CLVJPc_90vsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "8a4064a9-4bee-4954-8cb2-b7e7b9987614"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/homework2/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = BASE_PATH + 'harry_potter/'\n",
        "\n",
        "!pwd\n",
        "!ls\n",
        "os.chdir(BASE_PATH)\n",
        "if not os.path.exists(DATA_PATH + 'harry_potter.txt'):\n",
        "    !wget http://pjreddie.com/media/files/homework2.tar.gz\n",
        "    !tar -zxvf homework2.tar.gz\n",
        "    !rm homework2.tar.gz\n",
        "import pt_util\n",
        "os.chdir('/content')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hd1Qx66s19Pl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import pickle\n",
        "import re\n",
        "sys.path.append(BASE_PATH)\n",
        "import pt_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AxIvm7h62tfx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: Preprocessing the data\n",
        "In previous homeworks, we have provided a cleaned version of the data. But this time you'll have to do some of that cleaning yourselves.\n",
        "\n",
        "Hints:\n",
        "- train_text and test_text should contain the class indices for the character tokens from the data file. For example, if the text was **`\"ABA CDBE\"`**, the token version would be a numpy array with contents `[0, 1, 0, 2, 3, 4, 1, 5]`\n",
        "- The harry_potter.txt file has weird spacing. You might want to replace all the whitespace characters (space, \\n, \\t, etc.) in the file with the space character.\n",
        "- You should output two files. One for training and one for testing. The training should be the first 80% of the characters.\n",
        "- voc2ind is a map from character to the index of the class for that character. There is no predefined vocabulary, but you will need to be consistent across all tasks that use the vocabulary. For the example above, the voc2ind would be `{'A': 0, 'B': 1, ' ': 2, 'C': 3, 'D': 4, 'E': 5}`\n",
        "- ind2voc is the inverse of voc2ind\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6oZq_S6k3GpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "def prepare_data(data_path):\n",
        "    with open(data_path) as f:\n",
        "        # This reads all the data from the file, but does not do any processing on it.\n",
        "        data = f.read()\n",
        "        data=data.replace('space',' ')\n",
        "        data=data.replace('\\n',' ')\n",
        "        data=data.replace('\\t',' ')\n",
        "##         data = data.split(\" \")\n",
        "#         freqWord = {}\n",
        "#         iterCount = -1\n",
        "#         for word in data:\n",
        "#           iterCount += 1\n",
        "#           if word in freqWord:\n",
        "#             existing = freqWord[word]\n",
        "#             existing.append(iterCount)\n",
        "#             freqWord[word] = existing\n",
        "#           else:\n",
        "#             freqWord[word] = [iterCount]\n",
        "#         for word, wordCount in freqWord.items():\n",
        "#           if len(wordCount) < 5:\n",
        "#             for ind in wordCount:\n",
        "#               data[ind] = \"<unknown>\"\n",
        "        uniqList=list(set(data))\n",
        "    # TODO Add more preprocessing\n",
        "    length= len(data)\n",
        "    train_data= math.floor(0.8*(length))\n",
        "    voc2ind = {i:iterOn for iterOn,i in enumerate(uniqList)}\n",
        "    ind2voc = {val: key for key, val in voc2ind.items()}\n",
        "    train_text = data[0:train_data]\n",
        "    test_text = data[train_data:]\n",
        "    pickle.dump({'tokens': train_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_train.pkl', 'wb'))\n",
        "    pickle.dump({'tokens': test_text, 'ind2voc': ind2voc, 'voc2ind':voc2ind}, open(DATA_PATH + 'harry_potter_chars_test.pkl', 'wb'))\n",
        "    \n",
        "prepare_data(DATA_PATH + 'words.txt')\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, data_file):\n",
        "        with open(data_file, 'rb') as data_file:\n",
        "            dataset = pickle.load(data_file)\n",
        "        self.ind2voc = dataset['ind2voc']\n",
        "        self.voc2ind = dataset['voc2ind']\n",
        "\n",
        "    # Returns a string representation of the tokens.\n",
        "    def array_to_words(self, arr):\n",
        "##         return ' '.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "        return ''.join([self.ind2voc[int(ind)] for ind in arr])\n",
        "\n",
        "    # Returns a torch tensor representing each token in words.\n",
        "    def words_to_array(self, words):\n",
        "        return torch.LongTensor([self.voc2ind[word] for word in words])\n",
        "\n",
        "    # Returns the size of the vocabulary.\n",
        "    def __len__(self):\n",
        "        return len(self.voc2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzX1tUv8ilYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3: Loading the data\n",
        "This is possibly the trickiest part of this homework. In the past, batches were not correlated with each other, and the data within a single minibatch was also not correlated, so you could basically draw randomly from the dataset. That is not the case here. Instead, you should return sequences from the dataset.\n",
        "\n",
        "Your instructions are to implement the following. First, imagine splitting the dataset into N chunks where N is the batch_size and the chunks are contiguous parts of the data. For each batch, you should return one sequence from each of the chunks. The batches should also be sequential an example is described below.\n",
        "\n",
        "The data is 20 characters long `[1, 2, 3, ...20]`. The batch size is 2 and the sequence length is 4\n",
        "- The 1st batch should consist of  `(data =  [[1, 2, 3, 4]; [11, 12, 13, 14]], labels = [[2, 3, 4, 5]; [12, 13, 14, 15]])`\n",
        "- The 2nd batch should consist of `(data =  [[5, 6, 7, 8]; [15, 16, 17, 18]], labels = [[6, 7, 8, 9]; [16, 17, 18, 19]])`\n",
        "- The 3rd batch should consist of `(data =  [[9]; [19]], labels = [[10]; [20]])`\n",
        "- There is no 4th batch.\n",
        "\n",
        "Hints:\n",
        "- To work with the rest of the code, your len(dataset) should be a multiple of the batch_size. \n",
        "- Removing the last bit to make the data the proper shape will probably give better results than padding with 0s.\n",
        "- It is OK to have one batch be shorter than the others as long as all entries in that batch are the same length.\n",
        "- Notice that the last label in one batch is the first data in the next batch. Be careful of off-by-one errors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "44v6o0JwiwXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HarryPotterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_file, sequence_length, batch_size):\n",
        "        super(HarryPotterDataset, self).__init__()\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(data_file)\n",
        "        with open(data_file, 'rb') as data_pkl:\n",
        "            self.datas = pickle.load(data_pkl)\n",
        "        self.dataset= self.datas['tokens']\n",
        "        self.batchCount= math.floor(len(self.dataset)/(self.batch_size*self.sequence_length))\n",
        "        self.batch=1\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO return the number of unique sequences you have, not the number of characters.\n",
        "        return self.batchCount*self.batch_size\n",
        "          \n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        # Return the data and label for a character sequence as described above.\n",
        "        # The data and labels should be torch long tensors.\n",
        "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
        "        # in and how far down in the chunk you are.\n",
        "        location= idx*self.sequence_length\n",
        "        data=self.vocab.words_to_array(self.dataset[location:location+self.sequence_length])\n",
        "        label=self.vocab.words_to_array(self.dataset[location+1:location+1+self.sequence_length])\n",
        "        return (data,label)\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kYKDZoj2jCV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4: Defining the Network\n",
        "This time we will provide a network that should already get pretty good performance. You will still need to write the forward pass and inference functions. You may also choose to modify the network to try and get better performance.\n",
        "\n",
        "__BE CAREFUL:__ We have specified that the data will be fed in as batch_first. Look at the documentation if you are confused about the implications of this as well as how to call it for the forward pass. https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "mO21UXLj2ixn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 1.5\n",
        "\n",
        "class HarryPotterNet(nn.Module):\n",
        "    def __init__(self, vocab_size, feature_size):\n",
        "        super(HarryPotterNet, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.feature_size = feature_size\n",
        "        self.encoder = nn.Embedding(self.vocab_size, self.feature_size)\n",
        "        self.gru = nn.GRU(self.feature_size, self.feature_size, num_layers=2, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.feature_size, self.vocab_size)\n",
        "        \n",
        "        # This shares the encoder and decoder weights as described in lecture.\n",
        "        self.decoder.weight = self.encoder.weight\n",
        "        self.decoder.bias.data.zero_()\n",
        "        \n",
        "        self.best_accuracy = -1\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        batch_size = x.shape[0]\n",
        "        sequence_length = x.shape[1]\n",
        "        \n",
        "        # TODO finish defining the forward pass.\n",
        "        # You should return the output from the decoder as well as the hidden state given by the gru.\n",
        "        output, hidden_state = self.gru(self.encoder(x).view(batch_size, sequence_length, -1), hidden_state)\n",
        "        x = self.decoder(output)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
        "    def inference(self, x, hidden_state=None, temperature=1):\n",
        "        x = x.view(-1, 1)\n",
        "        x, hidden_state = self.forward(x, hidden_state)\n",
        "        x = x.view(1, -1)\n",
        "        x = x / max(temperature, 1e-20)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x, hidden_state\n",
        "\n",
        "    # Predefined loss function\n",
        "    def loss(self, prediction, label, reduction='elementwise_mean'):\n",
        "        loss_val = F.cross_entropy(prediction.view(-1, self.vocab_size), label.view(-1), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    # Saves the current model\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "\n",
        "    # Saves the best model so far\n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if accuracy > self.best_accuracy:\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "            self.best_accuracy = accuracy\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEQZIoB0jY5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 5: Character Generation\n",
        "\n",
        "In class we discussed three algorithms for creating sequences.\n",
        "1. Max: Choose the most likely value\n",
        "2. Sample: Sample from the distribution output by the network.\n",
        "3. Beam Search: Sample from the distribution and use the Beam Search algorithm.\n",
        "\n",
        "The beam search algorithm is as follows:\n",
        "```\n",
        "1. Initialize the beam list with the single existing empty beam\n",
        "2. Repeat for the sequence length:\n",
        "    1. For each beam in the beam list:\n",
        "        1. Compute the next distribution over the output space for that state\n",
        "        2. Sample from the distribution with replacement\n",
        "        3. For each sample:\n",
        "            1. Compute its score\n",
        "            2. Record its hidden state and chosen value\n",
        "        4. Add all the samples to the new beam list      \n",
        "     2. Rank the new beam list\n",
        "     3. Throw out all but the top N beams\n",
        " 3. Return the top beam's chosen values.\n",
        "```\n",
        "\n",
        "\n",
        "Hints:\n",
        "- np.random.choice and torch.multinomial will both help with the sampling as they can take in a weighted probability distribution and sample from that distribution.\n",
        "- For beam search you will need to keep a running score of the likelihood of each sequence. If you multiply the likelihoods, you will encounter float underflow. Instead, you should add the log likelihoods.\n",
        "- For beam search, you will need to keep track of multiple hidden states related to which branch you are currently expanding.\n",
        "- For beam search, you should search over the beam, but only return the top result in the end.\n",
        "- It may be useful to do the training part before the character generation part so you have some model to test.\n",
        "- Feel free to play around with the `BEAM_WIDTH`.\n"
      ]
    },
    {
      "metadata": {
        "id": "9XTxy4eq3UYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BEAM_WIDTH = 10\n",
        "\n",
        "def generate_language(model, device, seed_words, sequence_length, vocab, sampling_strategy='max', beam_width=BEAM_WIDTH):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        seed_words_arr = vocab.words_to_array(seed_words)\n",
        "\n",
        "        # Computes the initial hidden state from the prompt (seed words).\n",
        "        hidden = None\n",
        "        for ind in seed_words_arr:\n",
        "            data = ind.to(device)\n",
        "            output, hidden = model.inference(data, hidden)\n",
        "\n",
        "        outputs = []\n",
        "        # Initializes the beam list.\n",
        "        beam = [([], output, hidden, 0)]\n",
        "        \n",
        "        for ii in range(sequence_length):\n",
        "\n",
        "            if sampling_strategy == 'max':\n",
        "                # TODO max sampling strategy\n",
        "                indexOn = -1\n",
        "                maxVal = -1\n",
        "                current = -1\n",
        "                for out in output[0,:]:\n",
        "                  current += 1\n",
        "                  if out > maxVal:\n",
        "                    maxVal = out\n",
        "                    indexOn = current\n",
        "                outputs.append(torch.tensor(indexOn))            \n",
        "                output, hidden = model.inference(outputs[-1].to(device), hidden)\n",
        "            elif sampling_strategy == 'sample':\n",
        "                # TODO: Probability-based sampling strategy.\n",
        "                indexOn = torch.multinomial(output.data.view(-1), 1)[0]\n",
        "                outputs.append(torch.tensor(indexOn))            \n",
        "                output, hidden = model.inference(outputs[-1].to(device), hidden)\n",
        "            elif sampling_strategy == 'beam':\n",
        "              newbeam = []\n",
        "              for x in beam:\n",
        "                beamlist,output,hidden,score = x\n",
        "                output = output.view(-1)\n",
        "                multSamp = torch.multinomial(output,beam_width,replacement = True)\n",
        "                for y in multSamp:\n",
        "                  newout = []\n",
        "                  newout.extend(beamlist)\n",
        "                  scoreFound = torch.log(output[y])\n",
        "                  out,hiddenTemp = model.inference(y,hidden)\n",
        "                  output = output.view(-1)\n",
        "                  newout.append(y)\n",
        "                  newbeam.append([newout,out,hiddenTemp,scoreFound + score])\n",
        "              newbeam.sort(key=lambda x: x[3], reverse=True)\n",
        "              newbeam = newbeam[:beam_width]\n",
        "              beam = newbeam\n",
        "              if ii == sequence_length - 1:\n",
        "                outputs = beam[0][0]\n",
        "                  \n",
        "\n",
        "        return vocab.array_to_words(seed_words_arr.tolist() + outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Havsk_RJi_i5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 6: Training\n",
        "Again, we are providing training code for you. Have a look at the train function though as it implements the exact forward approximate backward computation, which may be of interest to you. You will still need to add the perplexity computation (read more in part 9 about how to do this)."
      ]
    },
    {
      "metadata": {
        "id": "L0Wq8hRy0UEX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    hidden = None\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        # Separates the hidden state across batches. \n",
        "        # Otherwise the backward would try to go all the way to the beginning every time.\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        pred = output.max(-1)[1]\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden = None\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            test_loss += model.loss(output, label, reduction='elementwise_mean').item()\n",
        "            pred = output.max(-1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            # Comment this out to avoid printing test results\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Input\\t%s\\nGT\\t%s\\npred\\t%s\\n\\n' % (\n",
        "                    train_loader.dataset.vocab.array_to_words(data[0]),\n",
        "                    train_loader.dataset.vocab.array_to_words(label[0]),\n",
        "                    train_loader.dataset.vocab.array_to_words(pred[0])))\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = 100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * test_loader.dataset.sequence_length,\n",
        "        100. * correct / (len(test_loader.dataset) * test_loader.dataset.sequence_length)))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66T-Ylkg0fn1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2132
        },
        "outputId": "e479428e-96e2-4347-9570-1748d542ecb9"
      },
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 256\n",
        "FEATURE_SIZE = 512\n",
        "TEST_BATCH_SIZE = 256\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.002\n",
        "WEIGHT_DECAY = 0.0005\n",
        "USE_CUDA = True\n",
        "PRINT_INTERVAL = 10\n",
        "LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "\n",
        "\n",
        "data_train = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_train.pkl', SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "data_test = HarryPotterDataset(DATA_PATH + 'harry_potter_chars_test.pkl', SEQUENCE_LENGTH, TEST_BATCH_SIZE)\n",
        "vocab = data_train.vocab\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                          shuffle=False, **kwargs)\n",
        "\n",
        "model = HarryPotterNet(data_train.vocab_size(), FEATURE_SIZE).to(device)\n",
        "\n",
        "# Adam is an optimizer like SGD but a bit fancier. It tends to work faster and better than SGD.\n",
        "# We will talk more about different optimization methods in class.\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        "expTrainLoss = []\n",
        "expTestLoss = []\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH, ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "expTestLoss.append((start_epoch, math.exp(test_loss)))\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "        train_loss = train(model, device, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        expTrainLoss.append((epoch, math.exp(train_loss)))\n",
        "        expTestLoss.append((epoch, math.exp(test_loss)))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH, (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, DATA_PATH + 'checkpoints/%03d.pt' % epoch)\n",
        "#         seed_words = 'Harry Potter'.split(\" \")\n",
        "        seed_words = 'Harry Potter'\n",
        "        for ii in range(10):\n",
        "            generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'sample')\n",
        "            print('generated sample\\t', generated_sentence)\n",
        "        generated_sentence = generate_language(model, device, seed_words, 200, vocab, 'beam')\n",
        "        print('generated beam\\t\\t', generated_sentence)\n",
        "        print('')\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print('Saving final model')\n",
        "    model.save_model(DATA_PATH + 'checkpoints/%03d.pt' % epoch, 0)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num workers: 2\n",
            "Input\tn south-seeking south-side south-southeast south-south-east south-southeasterly south-southeastward \n",
            "GT\t south-seeking south-side south-southeast south-south-east south-southeasterly south-southeastward s\n",
            "pred\tmtt28GggggZZZbtttj8GgggG'ZZj88Ggggg8Ggg511MT8HGgggg8Ggggk11MTTHGgggg8Ggg5111166008HHgggg8Ggg511G1N11\n",
            "\n",
            "\n",
            "Input\td tawnie tawnier tawnies tawniest tawny-faced tawny-gold tawny-gray tawny-green tawny-haired tawny-y\n",
            "GT\t tawnie tawnier tawnies tawniest tawny-faced tawny-gold tawny-gray tawny-green tawny-haired tawny-ye\n",
            "pred\tkMMMMdbbjMMvdbKKKGppNbKjjjMpGbKjGMGMMGLSokkk''MMpdS'SS''8MMMdS'SS4StpMpaLSSSkZZZGMpNLSSokKKssqMpdSSS\n",
            "\n",
            "\n",
            "Input\tnattributably unattributed unattributive unattributively unattributiveness unattuned unau unauctione\n",
            "GT\tattributably unattributed unattributive unattributively unattributiveness unattuned unau unauctioned\n",
            "pred\t44GGGGeGGG1b''HtbGGGGeGGGnj8ddGGGGGGGGGGjdddGGGGGGGGGG560HtbGGGGeGGGGGGdZ22HGGGGGGGnjdddddddddXGG8Gb\n",
            "\n",
            "\n",
            "Input\t vainness vainnesses Vaios vair vairagi vaire vairee vairy vairs Vaish Vaisheshika Vaishnava Vaishna\n",
            "GT\tvainness vainnesses Vaios vair vairagi vaire vairee vairy vairs Vaish Vaisheshika Vaishnava Vaishnav\n",
            "pred\tddMMbbdj22HHHbGdj2222zHqjjjHHMHa0HM4444tMMM,4j0HMH4nZ0HHHK00HHHHjq44jjtq4qjj526EZvjj4bjjtLdcMM4bjjtL\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 8.1058, Accuracy: 13965/947200 (1%)\n",
            "\n",
            "Train Epoch: 0 [0/38656 (0%)]\tLoss: 7.871111\n",
            "Train Epoch: 0 [2560/38656 (7%)]\tLoss: 15.247710\n",
            "Train Epoch: 0 [5120/38656 (13%)]\tLoss: 10.407890\n",
            "Train Epoch: 0 [7680/38656 (20%)]\tLoss: 8.314883\n",
            "Train Epoch: 0 [10240/38656 (26%)]\tLoss: 7.265882\n",
            "Train Epoch: 0 [12800/38656 (33%)]\tLoss: 6.072417\n",
            "Train Epoch: 0 [15360/38656 (40%)]\tLoss: 6.145309\n",
            "Train Epoch: 0 [17920/38656 (46%)]\tLoss: 4.154677\n",
            "Train Epoch: 0 [20480/38656 (53%)]\tLoss: 6.112937\n",
            "Train Epoch: 0 [23040/38656 (60%)]\tLoss: 3.644019\n",
            "Train Epoch: 0 [25600/38656 (66%)]\tLoss: 3.512801\n",
            "Train Epoch: 0 [28160/38656 (73%)]\tLoss: 2.874650\n",
            "Train Epoch: 0 [30720/38656 (79%)]\tLoss: 3.215192\n",
            "Train Epoch: 0 [33280/38656 (86%)]\tLoss: 3.157335\n",
            "Train Epoch: 0 [35840/38656 (93%)]\tLoss: 3.476653\n",
            "Train Epoch: 0 [38400/38656 (99%)]\tLoss: 3.110096\n",
            "Input\tn south-seeking south-side south-southeast south-south-east south-southeasterly south-southeastward \n",
            "GT\t south-seeking south-side south-southeast south-south-east south-southeasterly south-southeastward s\n",
            "pred\tesDnnnnnonn ng seknkookkonrknn-oooutkotd  rkkkkoututtttttturkkt--ttttttttt r   sttttootuttottt ot  s\n",
            "\n",
            "\n",
            "Input\td tawnie tawnier tawnies tawniest tawny-faced tawny-gold tawny-gray tawny-green tawny-haired tawny-y\n",
            "GT\t tawnie tawnier tawnies tawniest tawny-faced tawny-gold tawny-gray tawny-green tawny-haired tawny-ye\n",
            "pred\teseseeaasaneea  seseen  nennenn  senee wwnwr sanne fana ----e-gaannnenne gaaantsa-ee nonnon ssnne nn\n",
            "\n",
            "\n",
            "Input\tnattributably unattributed unattributive unattributively unattributiveness unattuned unau unauctione\n",
            "GT\tattributably unattributed unattributive unattributively unattributiveness unattuned unau unauctioned\n",
            "pred\twdoeeoon nee sstteeeuunur ssttuuuuunuue utttuuuuuuuuu   utttuuuuunuuu uu  rnttuune  sntttnnttnuuuuuu\n",
            "\n",
            "\n",
            "Input\t vainness vainnesses Vaios vair vairagi vaire vairee vairy vairs Vaish Vaisheshika Vaishnava Vaishna\n",
            "GT\tvainness vainnesses Vaios vair vairagi vaire vairee vairy vairs Vaish Vaisheshika Vaishnava Vaishnav\n",
            "pred\tuuunuuu  nenngun  n vvnnnnnennessssss nsessssssssssssasno sesne senneosSsnk    k  shsskkokiksSsnkkok\n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 3.3115, Accuracy: 220005/947200 (23%)\n",
            "\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints/000.pt\n",
            "\n",
            "generated sample\t Harry Potter sevuntter setruto-utter reut rul-utt-urettur -utut-ruuntututruu nutuunutunuutuuutuunuuutuuutuuuunutuuuntuuuuutuunuuuutuuuuuuutuuuuuruuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potter sermatou sel ronoustroul onounatuonu ronuurnouuntuunouuntuuounutunuuuntuuununtuuunuuunuuunuuunuunuuuuunuuuuuunuuuuunuuuuuuuuuuuuunuuuuuuuunuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potterus re-outer rountture ututneruuntuturuunttuuutnuutunutuuutuuuntuuutuununuutuuuuuuntuuuuunutuuuuuuntuuuuuuuunuuuuuunuuuuuuuuunuuuuuuuuuuuuuuQuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potter seruterouter seturetiount seruturtuuttureuurtuututuuuttutuuutuutuutuuutuuutuuuutuuutuuuuutuuunuutuuutuuutuuuutuuuuunuuuuuuuuuuuuuuuuuuuuuumuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potteruer setruont ruseruntur unturutuntuuutturuutuutuunutuutuuutuuruuuntuuuuutuuuunuuutuuuuutuuuruuuuuuuuutuuuuuuuruuuuuuuuuuuunuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potterm rsout-turmitut ruututdutuutulrutuntuuourutuly uutuumutuuuunutuunuutuuutuuuunuuuutuuuuunuuuuuuuuuuuunuuuuuuuuuuuuuuuunuuuuuunuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potter soremitor seoptreoun selvou-torunutoun durenuloun uuntul-ruulutununu nuuunuunuuntuuruunuuunuuunutuuuunuuuunuuunuuuzuuuutuuuunuuuuuuuunuuuuuunuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potter sunteruter skerettur seltur -roututter uutuntuentuuttturu uutututuutuuututuutuutuumutuuutuutuuuutuutuuutuuutuuuuuutuuutuuuuutuuuuuutunuuuuuuuuutuuuuuuuuuutuuuuuuuuuutuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Potter seoron setutorentut orunttuted nutunttuun utungutununtuuuntuutununuutuuuntuutuuunuuuunutuuuuunuuutuuuuunuuuuuuunuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\n",
            "generated sample\t Harry Pottera seroven -opranedos ne-ponoron u-nonounedo nuononu nounonunonoununonuunonunounouunonunuonuunounuonuunounuounouununouunuounuounuzouununuuunouuunuuzuunouuuunuuuzuuuunuuuuunuuuunuuuunuuuuuuuuuuuuuunuuuu\n",
            "generated beam\t\t Harry Potter seroutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter serutter \n",
            "\n",
            "Train Epoch: 1 [0/38656 (0%)]\tLoss: 3.912137\n",
            "Train Epoch: 1 [2560/38656 (7%)]\tLoss: 3.311208\n",
            "Train Epoch: 1 [5120/38656 (13%)]\tLoss: 2.837013\n",
            "Train Epoch: 1 [7680/38656 (20%)]\tLoss: 2.652254\n",
            "Train Epoch: 1 [10240/38656 (26%)]\tLoss: 2.616056\n",
            "Train Epoch: 1 [12800/38656 (33%)]\tLoss: 2.766356\n",
            "Train Epoch: 1 [15360/38656 (40%)]\tLoss: 3.495226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-8:\n",
            "Process Process-7:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Interrupted\n",
            "Saving final model\n",
            "Saved /gdrive/My Drive/colab_files/homework2/harry_potter/checkpoints/001.pt\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r597GUTVjwZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 7: Experiments"
      ]
    },
    {
      "metadata": {
        "id": "zgLylYlp9kBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "a8d1842e-6eb9-4968-beab-3e9851a3ba0e"
      },
      "cell_type": "code",
      "source": [
        "# seed_words = 'Harry Potter and the'.split(\" \")\n",
        "seed_words = 'Harry Potter and the'\n",
        "sequence_length = 200\n",
        "\n",
        "generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'max')\n",
        "print('generated with max\\t', generated_sentence)\n",
        "\n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'sample')\n",
        "    print('generated with sample\\t', generated_sentence)\n",
        "    \n",
        "for ii in range(10):\n",
        "    generated_sentence = generate_language(model, device, seed_words, sequence_length, vocab, 'beam')\n",
        "    print('generated with beam\\t', generated_sentence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated with max\t Harry Potter and there rethernerenterenterenterentering retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting retenting rete\n",
            "generated with sample\t Harry Potter and thearteri rektermentrest reStermenturent reminettereting remintite remintertednecterentinerentientinus reminterrenention remintrementencerenceRcencementirescencenement remincentricism remincementcker rem\n",
            "generated with sample\t Harry Potter and theea rontheaman rontement rontmentimeron ront&mentinner rontennererment rontenneromonhy rontennmentorum rontententonmenting rentmentersentiontent rentent'ment rentelonness rentenerntry rentineneing rent\n",
            "generated with sample\t Harry Potter and thero rhtenonered rnteinnered rentienting rentinetimonient rentinement's rentiments rentimmentitene rentmictrementing rentminerentlenescenticent rentimptoniceneesmentiium rentminemention rentmentoprement\n",
            "generated with sample\t Harry Potter and theidder Shindo rhindonine rhindenninzing rhindenting rundentian rundenedly rinendenting rinnetmentiferountent-mantering rinnettinmentrent rinneumentionRenension rinnentize rinnennizerentenenerenemencenc\n",
            "generated with sample\t Harry Potter and theneo retnennness rentnementioneementinestementiterementine rentomentiseness rentomoneticric renttomnesserinerences rententitoning renttingentinessereine rentictentenenesse renticenent'sentfuerorteugh r\n",
            "generated with sample\t Harry Potter and theer Phenter-hood LinteR ryntRR rintundementer rinntenneted rintnennestenienkentinerent rintent-nonenenterenting rinententingery rintenterententling rintent-rampernite rintennormentRs rinnetic rinnertie\n",
            "generated with sample\t Harry Potter and theTs Serth Server Server Servers server serverter serverterverch serverchaverer servercerteroom serce-reveleptrered server-leveded serverleenvered sepverily sepverigerence seviererpragevellegererically \n",
            "generated with sample\t Harry Potter and theretmen Rhettine rhenticinent rhentinerencensintenentlerincenencestringentimenerinencementricencarientlenteremellerio renemiscribeier remenyieridementry remientimenement remimentismentramentums remimin\n",
            "generated with sample\t Harry Potter and thermance shiptan-beregrecemencemence rinteferne rinthener rinnentererenteriententerenerenceringne-ferentineresmerrienting rineneremine rinenemient rineneriomininerencie rineneromentrimineneremicterine r\n",
            "generated with sample\t Harry Potter and theria Rhotaceae rohtabeientienceneses Rontenism rontentibized rontennetist rontentenism rontention rontenembation rontenemoon rontenement ronmenteranous rontentimment rentrentimentoomingnement rentinmen\n",
            "generated with beam\t Harry Potter and thearter rethenting rethneting rethnetingnentinentineness retnenting rentinenting rentinenting rentinenting rentinentineness rentinentineness rentinentineness rentinentineness rentinentineness rentinenti\n",
            "generated with beam\t Harry Potter and thearter rethenting rethneting rethnetingnentinentineness retnenting rentinenting rentinenting rentinenting rentinentineness rentinentineness rentinentineness rentinentineness rentinentineness rentinenti\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-47903331e6af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgenerated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generated with beam\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0d06be157288>\u001b[0m in \u001b[0;36mgenerate_language\u001b[0;34m(model, device, seed_words, sequence_length, vocab, sampling_strategy, beam_width)\u001b[0m\n\u001b[1;32m     50\u001b[0m                   \u001b[0mnewout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                   \u001b[0mnewbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhiddenTemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoreFound\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m               \u001b[0mnewbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m               \u001b[0mnewbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewbeam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m               \u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pn0RWPBFjzkP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 8: Other things\n",
        "Choose **two** of the following to try. It will probably be beneficial to create new code cells below rather than modifying your earlier code:\n",
        "\n",
        "\n",
        "1. Train on a different text corpus. The corpus should be at least as large as the provided Harry Potter dataset.\n",
        "    - Options include other books, websites, tweets, wikipedia articles etc.\n",
        "    -  (Hint: this is probably the easiest one)\n",
        "2. Train a model using student-forcing.\n",
        "    - You will have to modify the network inputs.\n",
        "    - You will need to use `torch.nn.GRUCell` and its like. https://pytorch.org/docs/stable/nn.html#grucell\n",
        "    - You cannot simply feed an empty string to start off a sequence. The sequence must be somehow conditioned on prior ground truth.\n",
        "3. Train a model on words instead of characters.\n",
        "    - You will need to redefine your input/output space vocabulary as well.\n",
        "    - You should replace any words that occur less than 5 times in the dataset with an <unknown\\> token. \n",
        "4. Write a new data loader which picks a random point in the text to start from and returns 10 consecutive sequences starting from that point onward. \n",
        "    - You should also modify the train and test functions to reset the memory when you reset the sequence.\n",
        "    - You should consider an epoch to be feeding in approximately the number of characters in the dataset.\n",
        "    - You may run into issues if your dataset size/epochs are not a multiple of your batch size.\n",
        "5. Train on sentences instead of one long sequence.\n",
        "    - You should still produce output character by character.\n",
        "    - Sentences can end with a . ! ?, but words like Mr. generally do not end a sentence.\n",
        "    - A sentence may also continue in the case of quotations. For example: ``\"Do your homework!\" said the TAs.`` is only one sentence.\n",
        "    - Your parsing does not have to be perfect, but try to incorporate as many of these rules as you can.\n",
        "    - Feel free to use existing NLP tools for finding sentence endings. One is spacy: https://spacy.io/usage/linguistic-features#section-sbd\n",
        "    - All sentences should end with an <eos\\> token. Your output sampling should now stop when it produces the <eos\\> token.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vWMlB2U3onZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 9: Short answer questions\n",
        "Please answer these questions, and put the answers in a file called homework2_python.pdf in your repository.\n",
        "\n",
        "\n",
        "1. Just like last time, provide plots for training error, test error, and test accuracy. Also provide a plot of your train and test perplexity per epoch.\n",
        "    - In class we defined perplexity as `2^(p*log_2(q))`, However the PyTorch cross entropy function uses the natural log. To compute perplexity directly from the cross entropy, you should use `e^p*ln(q)`.\n",
        "    - We encourage you to try multiple network modifications and hyperparameters, but you only need to provide plots for your best model. Please list the modifications and hyperparameters.    \n",
        "    \n",
        "2. What was your final test accuracy? What was your final test perplexity?\n",
        "3. What was your favorite sentence generated via each of the sampling methods? What was the prompt you gave to generate that sentence?\n",
        "4. Which sampling method seemed to generate the best results? Why do you think that is?\n",
        "5. For sampling and beam search, try multiple temperatures between 0 and 2. \n",
        "    - Which produces the best outputs? Best as in made the most sense, your favorite, or funniest, doesn't really matter how you decide.\n",
        "    - What does a temperature of 0 do? What does a temperature of 0<temp<1 do? What does a temperature of 1 do? What does a temperature of above 1 do? What would a negative temperature do (assuming the code allowed for negative temperature)?\n",
        "    \n",
        "Questions for each of the \"Other things\" sections. Only answer the questions corresponding to the ones you chose.\n",
        "\n",
        "1. New Corpus\n",
        "    1. What corpus did you choose? How many characters were in it?\n",
        "    2. What differences did you notice between the sentences generated with the new/vs old corpus.\n",
        "    3. Provide outputs for each sampling method on the new corpus (you can pick one temperature, but say what it was).\n",
        "\n",
        "2. Student-forcing\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. Were the results better than with teacher-forcing?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was).\n",
        "    \n",
        "3. Words\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. How large was your vocabulary?\n",
        "    3. Did you find that different batch size, sequence length, and feature size and other hyperparameters were needed? If so, what worked best for you?\n",
        "\n",
        "4. Random Dataloader\n",
        "    1. What new difficulties did you run into while training?\n",
        "    2. Were the results better than with the original dataloader?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was). \n",
        "    \n",
        "5. Sentences\n",
        "    1. What new difficulties did you run into while training? What new difficulties did you run into while preprocessing?\n",
        "    2. Were the results better than with the original dataloader?\n",
        "    3. Provide some outputs for each sampling method (you can pick one temperature, but say what it was). \n",
        "\n",
        "\n",
        "    "
      ]
    }
  ]
}